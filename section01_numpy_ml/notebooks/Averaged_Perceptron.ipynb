{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae0v4yrZgvQt"
      },
      "source": [
        "<h1>Averaged Perceptron</h1>\n",
        "The Perceptron is an interesting algorithm that lets us classify data by seperating labeled data with a hyperplane (in low dimensions this is just a 2D line or a 3D plane). You've also seen how to implement your own Perceptron in code to classify flowers based off of some measured \"features\" (also known as \"handcrafted\" features). As you may have noticed, the Perceptron struggels to converge with this data and tends to occilate. A more stable version of the algorithm is the \"Averaged\" Perceptron, which creates a weighted average of the calculated weights based off of how many correct predictions a vector of weights gets in a row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Pbx7ppsgvQv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd # you may need to install this!\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et5VQL7pgvQv"
      },
      "source": [
        "We will use the Iris flower dataset, which happens to be one of the first datasets created for statistical analysis. The Iris dataset contains 150 examples of Iris flowers belonging to 3 species *Iris-setosa*, *Iris-versicolor* and, *Iris-virginica*. Each example has 4 features *sepal length*, *sepal width*, *petal length*, and *petal width*. See the image below for an illustration.\n",
        "\n",
        "Below, we use the Pandas package to download and read the dataset first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZlJY1VGgvQv"
      },
      "outputs": [],
      "source": [
        "#Download url\n",
        "URL_='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "#Use URL to create a Pandas dataframe\n",
        "data = pd.read_csv(URL_, header = None)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4TSFDmgvQw"
      },
      "source": [
        "So, the data of the **first** 5 examples looks as follows:\n",
        "\n",
        "| exmaple# | sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | target name|\n",
        "| --- | --- | --- || --- | --- |\n",
        "| 0 | 5.1 | 3.5 | 1.4 |  0.2|  Iris-setosa\n",
        "| 1 |4.9|  3. |  1.4|  0.2|  Iris-setosa\n",
        "| 2 |4.7|  3.2|  1.3|  0.2|  Iris-setosa\n",
        "| 3 |4.6|  3.1|  1.5|  0.2|  Iris-setosa\n",
        "| 4 |5. |  3.6|  1.4|  0.2|  Iris-setosa\n",
        "\n",
        "\n",
        "We consider the Iris-setosa as positive class and Iris-versicolor as negative (they are the entries from 1-100 but remember in Python arrays start from index 0). For visulization purposes, we will use the first two features (*i.e.*, sepal length, sepal width) in our experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLv5scSUgvQy"
      },
      "outputs": [],
      "source": [
        "#There are 50 setosa and 50 versicolor and 50 virginica, we are just using setosa and versicolor\n",
        "#make the dataset linearly separable\n",
        "#aka convert the target names to -1 or 1 so we can train with it\n",
        "data[4] = np.where(data.iloc[:, -1]=='Iris-setosa', 1, -1)\n",
        "\n",
        "#Convert Pandas dataframe to a Numpy array\n",
        "np_data = np.asarray(data, dtype = 'float64')\n",
        "\n",
        "#We will train the Perceptron using the first two attribute sepal length and sepal width\n",
        "x_train = np_data[:100,:2]\n",
        "#ouput is the target name which we converted into either 1 or -1\n",
        "y_train = np_data[:100,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVPpdlTwgvQy"
      },
      "source": [
        "Before training the perceptron, let us visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBXY4bTVgvQy"
      },
      "outputs": [],
      "source": [
        "# scattter plot\n",
        "scatter = plt.scatter(x_train[:, 0], x_train[:, 1],\n",
        "            s=200, c=np.squeeze((y_train+1)/2),\n",
        "            marker='x', cmap=mpl.colors.ListedColormap([\"blue\", \"green\"]))\n",
        "\n",
        "plt.xlabel('sepal lenght', size=20)\n",
        "plt.ylabel('sepal width', size=20)\n",
        "\n",
        "plt.legend(scatter.legend_elements()[0], ['Iris-setosa', ' Iris-versicolor'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zryS9S6QgvQy"
      },
      "source": [
        "<h2>Implementing the Perceptron</h2>\n",
        "\n",
        "[Perceptron](https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm)\n",
        "\n",
        "First, re-implement the vanilla Perceptron using weights w and bias b.<br>\n",
        "After you have done this, update w_ave and b_ave as per the \"Averaged Perceptron\" algorithm<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s47O_47gvQy"
      },
      "outputs": [],
      "source": [
        "nSamples, dim = x_train.shape\n",
        "max_iter = 10\n",
        "acc_ave = 0\n",
        "acc = 0\n",
        "#init our parameters\n",
        "w = ####To Do!\n",
        "b = ####To Do!\n",
        "\n",
        "w_ave = ####To Do!\n",
        "b_ave = ####To Do!\n",
        "\n",
        "#We'll log the accuracy to plot later\n",
        "log_acc = []\n",
        "log_acc_ave = []\n",
        "\n",
        "#setting the random seed will generate the same sequence on every run, making it possible to reproduce randomization\n",
        "np.random.seed(10)\n",
        "#Perform a number of iterations over the whole dataset\n",
        "####To Do! Create a Loop that will perform the inner loop max_iter times!\n",
        "    c = ####To Do! Initialise a counter to 1\n",
        "    rnd_idx = ####To Do! #create an array of random indices created at the start of every epoch\n",
        "    ####To Do! Create a Loop that will iterate over rnd_idx returning one index at a time!\n",
        "\n",
        "        #Sample a single datapoint\n",
        "        x_i = ####To Do! Use the current index to select the current input TRAINING data point\n",
        "        y_i = ####To Do! Use the current index to select the current output TRAINING data point\n",
        "\n",
        "        #Calculate the prediction\n",
        "        y_hat = ####To Do!\n",
        "\n",
        "        #Only update the Perceptron if the prediction is incorrect\n",
        "        if ####To Do! Check if the Prediction is correct!\n",
        "            #if prediction is not correct update the ave-weights with the old weights\n",
        "            #before updating the weights\n",
        "\n",
        "            w_ave += ####To Do!\n",
        "            b_ave += ####To Do!\n",
        "\n",
        "            #Update the weights and bias\n",
        "            w += ####To Do!\n",
        "            b += ####To Do!\n",
        "\n",
        "            #reset counter\n",
        "            c = ####To Do!\n",
        "        else:\n",
        "            #if prediction is correct increment counter\n",
        "            c = ####To Do!\n",
        "\n",
        "    #Calculate the output for every sample using the average weights a bias\n",
        "    y_hat_X_ave = ####To Do!\n",
        "    #Compare to the real labels to calculate the accuracy\n",
        "    acc_ave = ####To Do!\n",
        "    log_acc_ave.append(acc_ave)\n",
        "\n",
        "    #Calculate the output for every sample\n",
        "    y_hat_X = ####To Do!\n",
        "    #Compare to the real labels to calculate the accuracy\n",
        "    acc = ####To Do!\n",
        "    log_acc.append(acc)\n",
        "\n",
        "    print(\"Iter{0}: accuracy: {1:.3f}, averaged accuracy: {2:.3f}\".format(cur_iter+1, acc, acc_ave))\n",
        "    #Note: this is another way you can format print statements in python (there are many ways)\n",
        "    #the ints 0,1,2 tell python which value from a list you want to print out\n",
        "    #.3f is the datatype (f - float) and precision (.3 - 3 decimal points) just like matlab!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pif2p4VigvQz"
      },
      "source": [
        "<h3>Results</h3>\n",
        "As we can see (and you might have gussed) this dataset is not perfectly seperable, as a result the vanilla perceptron occilates back and forth. The Averaged Perceptron however manages to converge to the best possible solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax4QeJ_vgvQz"
      },
      "outputs": [],
      "source": [
        "# scattter plot\n",
        "scatter2 = plt.scatter(x_train[:, 0], x_train[:, 1],\n",
        "        s=200, c=np.squeeze((y_train+1)/2),\n",
        "        marker='o', cmap=mpl.colors.ListedColormap([\"blue\", \"red\"]))\n",
        "\n",
        "scatter = plt.scatter(x_train[:, 0], x_train[:, 1],\n",
        "        s=200, c=np.squeeze((y_hat_X_ave+1)/2),\n",
        "        marker='x', cmap=mpl.colors.ListedColormap([\"blue\", \"red\"]))\n",
        "\n",
        "plt.xlabel('sepal lenght', size=20)\n",
        "plt.ylabel('sepal width', size=20)\n",
        "plt.title('Predicted vs Ground Truth', size=20)\n",
        "\n",
        "plt.legend(scatter.legend_elements()[0], ['Iris-setosa', ' Iris-versicolor'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i2UwtJpgvQz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 5))\n",
        "plt.plot(log_acc_ave)\n",
        "plt.plot(log_acc)\n",
        "plt.legend([\"Average\", \"Vanilla\"])\n",
        "\n",
        "plt.xlabel('Iteration', size=20)\n",
        "plt.ylabel('Accuracy', size=20)\n",
        "plt.title(\"Average vs Vanilla Perceptron\", size=30)\n",
        "# plt.savefig(\"perceptron.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}